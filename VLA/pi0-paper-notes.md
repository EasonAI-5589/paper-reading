# π₀: A Vision-Language-Action Flow Model for General Robot Control

> Physical Intelligence, 2024
> arXiv:2410.24164
> Kevin Black, Noah Brown, Danny Driess, Chelsea Finn, Sergey Levine 等

---

## Section 1: Motivation & Problem Definition

---

### 1.1 研究问题定义

#### 核心任务

> 如何构建一个**通用机器人基础模型 (Robot Foundation Model)**，能够通过预训练获得广泛的物理操作能力，并通过微调适应复杂灵巧任务？

#### 问题范畴

- **领域**: 机器人学习、具身智能、VLA (Vision-Language-Action) 模型
- **子领域**: 跨具身泛化、灵巧操作、机器人基础模型

#### 具体案例分析

##### 案例 1：NLP/CV 中的预训练范式成功

- **场景背景**: 在自然语言和计算机视觉领域，通用的预训练基础模型已取得巨大成功
- **现有做法**: 如识别鸟类图片，传统方法是收集鸟类数据单独训练
- **遇到的问题**: 数据量有限、泛化能力差
- **理想状态**: 先在海量图像-语言数据上预训练，再微调到鸟类识别，效果更好

##### 案例 2：机器人学习面临的核心挑战

- **场景背景**: 机器人需要在多样化物理环境中执行各种任务
- **现有做法**: 为每个任务单独收集数据、训练专用策略
- **遇到的问题**:
  - **数据稀缺**: 机器人数据收集成本高
  - **泛化不足**: 窄域数据无法覆盖真实场景的多样性
  - **鲁棒性差**: 专用数据缺乏错误恢复行为
- **理想状态**: 像 LLM 一样先在多样数据上预训练，获得广泛能力后再微调

##### 案例 3：现有 VLA 模型的局限

- **场景背景**: 已有 VLA 模型（如 RT-2, OpenVLA）尝试将 VLM 用于机器人控制
- **现有做法**: 使用自回归离散化表示动作 token
- **遇到的问题**:
  - 控制频率受限 (2-10 Hz)
  - 无法表示复杂连续动作分布
  - 难以处理高精度灵巧操作
- **理想状态**: 能够以 50Hz 频率控制，执行叠衣服、组装盒子等精细任务

#### 图片辅助说明

**Figure 1 分析**:
- **图示内容**: π₀ 整体架构和训练流程
- **关键信息**:
  - 左侧：多样化的跨具身预训练数据（多种机器人、68 个任务）
  - 中间：VLM backbone + Action Expert 架构
  - 右侧：复杂灵巧任务的执行效果（叠衣服、组装盒子）
- **揭示的洞察**: 预训练+微调范式可以从多样数据迁移到复杂单任务

**Figure 2 分析**:
- **图示内容**: π₀ 控制移动机械臂完成洗衣任务的完整流程
- **关键信息**: 从烘干机取衣物 → 放入篮子 → 运送到折叠台 → 逐件折叠
- **与案例对应**: 展示了 10-15 分钟的长时程复杂任务，传统方法难以实现

#### 问题的核心矛盾

1. **数据效率 vs 任务复杂度**: 灵巧操作需要大量高质量数据，但收集成本极高
2. **通用性 vs 精细控制**: 通用预训练模型难以保持高频精确控制
3. **离散化 vs 连续性**: 现有 VLA 的 token 离散化限制了动作表达能力

---

### 1.2 本文方法与核心创新

#### 总体方案

- **提出的方法**: π₀，一个基于 VLM 预训练 + Flow Matching 的通用机器人基础模型
- **核心 idea**: 用 **Flow Matching** 替代自回归离散化来生成连续动作分布，结合 **Action Expert** 与 VLM backbone 分离的 MoE 式架构

#### 关键创新点

##### 1. Flow Matching VLA 架构

- **是什么？** 首个将 Flow Matching（扩散的变体）与 VLM 预训练结合的 VLA 模型
- **解决了 1.1 中的哪个问题？** 解决了离散化 VLA 控制频率低、无法表示复杂连续分布的问题

##### 2. Action Expert 设计

- **是什么？** 300M 参数的独立模块，通过 MoE 式架构与 VLM backbone 分离
- **解决了 1.1 中的哪个问题？** 让 VLM 处理视觉语言理解，Action Expert 专注于高精度动作生成

##### 3. Pre-training/Post-training Recipe

- **是什么？** 借鉴 LLM 的训练范式，预训练获得广泛能力，后训练获得流畅高效执行
- **解决了 1.1 中的哪个问题？** 解决了数据稀缺和泛化不足的问题

##### 4. 跨具身训练 (Cross-Embodiment Training)

- **是什么？** 在 7 种机器人配置、68 个任务上统一训练单一模型
- **解决了 1.1 中的哪个问题？** 通过数据多样性提升泛化和鲁棒性

#### 问题-方案对应表

| 1.1 中的问题 | 本文的解决思路 |
|------------|---------------|
| 离散化限制控制频率 | Flow Matching 生成连续动作，支持 50Hz |
| VLA 难以处理灵巧操作 | Action Expert + Action Chunking (H=50) |
| 数据稀缺 | 跨具身预训练 + OXE 开源数据 |
| 泛化/鲁棒性不足 | 多样化预训练 + 高质量后训练 |

---

### 1.3 主要贡献与论文组织

#### 核心贡献总结

本文的主要贡献包括：

1. **新架构**: 提出 Flow Matching VLA，首次结合 VLM 预训练和流匹配的机器人基础模型
2. **新训练范式**: 提出 pre-training/post-training 分离的机器人训练策略
3. **大规模实验**: 在 10,000+ 小时机器人数据上训练，验证复杂灵巧任务（叠衣服、组装盒子）

#### 预期效果

- **关键性能指标**:
  - 开箱即用 (zero-shot) 在衬衫折叠任务达到 ~95% 成功率
  - 微调后在复杂任务（洗衣折叠）达到 ~75% 成功率
- **与 SOTA 对比**:
  - 比 OpenVLA 在所有任务上提升 3-10x
  - 预训练对困难任务帮助尤其大（有时提升 2x vs 从头训练）

---

## Section 2: Related Work

---

### 2.1 Vision-Language-Action Models (VLA)

#### 代表性工作对比

| 工作名称 | 核心方法 | 优势 | 局限 | 对应 1.1 的问题 |
|---------|---------|------|------|----------------|
| RT-2 | VLM + 离散化动作 token | ✓ 继承 VLM 语义知识 | ✗ 控制频率受限 (2-10Hz) | 离散化限制 |
| OpenVLA | 开源 VLA，自回归离散化 | ✓ 开源可复现 | ✗ 不支持 action chunking | 灵巧操作困难 |
| TinyVLA | 轻量化 VLA | ✓ 推理高效 | ✗ 能力有限 | 数据稀缺 |

> **一句话小结**: 现有 VLA 都使用自回归离散化，本质上无法表示高频连续动作分布。

---

### 2.2 Diffusion Models for Robot Control

#### 代表性工作对比

| 工作名称 | 核心方法 | 优势 | 局限 | 对应 1.1 的问题 |
|---------|---------|------|------|----------------|
| Diffusion Policy | 扩散模型生成动作 | ✓ 表示多模态动作分布 | ✗ 无 VLM 预训练 | 语义理解弱 |
| ACT | Action Chunking Transformer | ✓ 适合灵巧操作 | ✗ 需大量任务特定数据 | 数据稀缺 |
| Scaling DP to 1B | 扩散策略扩展到 1B 参数 | ✓ 规模大 | ✗ 无 VLM backbone | 语义泛化差 |

> **一句话小结**: 现有扩散策略能建模复杂动作，但缺乏互联网规模的语义预训练。

---

### 2.3 Large-Scale Robot Learning

#### 代表性工作对比

| 工作名称 | 核心方法 | 优势 | 局限 | 对应 1.1 的问题 |
|---------|---------|------|------|----------------|
| OXE Dataset | 22 种机器人的开源数据集 | ✓ 跨具身多样性 | ✗ 任务相对简单 | 复杂任务缺失 |
| DROID | 大规模野外数据收集 | ✓ 环境多样性 | ✗ 无复杂灵巧任务 | 灵巧操作缺失 |
| Bridge v2 | 桥接不同机器人的数据 | ✓ 跨平台迁移 | ✗ 规模有限 | 数据稀缺 |
| Octo | 93M 通用机器人策略 | ✓ 支持扩散 | ✗ 模型太小 | 能力受限 |

> **一句话小结**: 现有大规模数据集多为简单任务，π₀ 使用的是目前最大规模的灵巧操作数据集。

---

### 2.4 Hybrid Diffusion-Autoregressive Models

#### 代表性工作对比

| 工作名称 | 核心方法 | 优势 | 局限 | 对应 1.1 的问题 |
|---------|---------|------|------|----------------|
| Transfusion | 单 transformer 混合训练 | ✓ 统一架构 | ✗ 针对图像生成 | 需适配机器人 |
| MARS | MoE 式混合模型 | ✓ 高效专家 | ✗ 用于文图生成 | 需适配动作 |
| Playground v3 | 扩散+LLM 混合 | ✓ 结合两者优势 | ✗ 图像领域 | 需适配机器人 |

> **一句话小结**: 混合扩散-自回归模型在图像生成中已有探索，π₀ 将其首次应用于机器人控制。

---

### 2.5 本文定位

- **最相关工作**: Transfusion，本文与其的核心区别是将混合架构应用于机器人控制，并引入 Action Expert
- **主要 baseline**: OpenVLA 和 Octo，选择原因是它们是目前最具代表性的机器人基础模型
- **填补的空白**: 现有 VLA 无法以高频控制执行灵巧任务，本文通过 Flow Matching + Action Expert 解决

---

## Section 3: Method

---

### 3.0 Preliminary

#### 符号表

| 符号 | 含义 | 维度/取值 |
|-----|------|----------|
| $o_t$ | 观察 (observation) | 包含图像、语言、本体感知 |
| $I_t^i$ | 第 $i$ 个相机图像 | $H \times W \times 3$ |
| $\ell_t$ | 语言指令 token 序列 | 变长序列 |
| $q_t$ | 本体感知状态 (关节角度) | $\mathbb{R}^{d_q}$，最大 18 维 |
| $A_t$ | 动作块 (action chunk) | $[a_t, a_{t+1}, ..., a_{t+H-1}]$ |
| $H$ | 动作块长度 (horizon) | 50 |
| $\tau$ | Flow matching 时间步 | $\tau \in [0, 1]$ |
| $v_\theta$ | 学习的向量场 | 预测去噪方向 |

#### 基础概念

- **VLM (Vision-Language Model)**: 在互联网规模图像-文本数据上预训练的多模态模型
- **Flow Matching**: 扩散模型的变体，通过学习从噪声到数据的确定性流来生成样本
- **Action Chunking**: 一次预测 H 步未来动作，而非逐步预测

#### 论文使用的基础架构

> 本文基于 **PaliGemma** VLM 架构：
> - **SigLIP (400M)**: 图像编码器，将图像映射到与文本相同的嵌入空间
> - **Gemma 2B**: 语言模型骨干，提供语义理解和推理能力
> - 采用 late fusion 策略，图像 token 与文本 token 拼接后输入 transformer

---

### 3.1 模型架构概述

> **核心思想**: 在 VLM 基础上添加 Action Expert，通过 Flow Matching 生成连续动作分布

#### 架构示意

```
┌─────────────────────────────────────────────────────────────────────┐
│                           π₀ Architecture                           │
├─────────────────────────────────────────────────────────────────────┤
│                                                                     │
│  ┌─────────────┐   ┌─────────────┐   ┌─────────────┐               │
│  │   Image 1   │   │   Image 2   │   │   Image 3   │               │
│  └──────┬──────┘   └──────┬──────┘   └──────┬──────┘               │
│         │                 │                 │                       │
│         ▼                 ▼                 ▼                       │
│  ┌────────────────────────────────────────────────────┐             │
│  │              SigLIP Image Encoders                  │             │
│  └────────────────────────┬───────────────────────────┘             │
│                           │                                         │
│  ┌────────────────────────▼───────────────────────────┐             │
│  │            Concatenate: [Images; Language; State]   │             │
│  └────────────────────────┬───────────────────────────┘             │
│                           │                                         │
│  ┌────────────────────────▼───────────────────────────┐             │
│  │                  Transformer Backbone               │             │
│  │  ┌───────────────────┬───────────────────────────┐ │             │
│  │  │   VLM Backbone    │      Action Expert        │ │             │
│  │  │   (Gemma 2.6B)    │        (300M)             │ │             │
│  │  │                   │                           │ │             │
│  │  │  处理: I, ℓ       │  处理: q_t, A_t^τ         │ │             │
│  │  │  (图像+语言)      │  (状态+噪声动作)          │ │             │
│  │  └───────────────────┴───────────────────────────┘ │             │
│  └────────────────────────┬───────────────────────────┘             │
│                           │                                         │
│                           ▼                                         │
│  ┌────────────────────────────────────────────────────┐             │
│  │              Flow Matching Output: v_θ              │             │
│  │              (预测去噪向量场)                        │             │
│  └────────────────────────────────────────────────────┘             │
│                                                                     │
└─────────────────────────────────────────────────────────────────────┘
```

**参考图**: Figure 3

**整体流程**:
$[I_t^1, ..., I_t^n, \ell_t] \xrightarrow{\text{VLM Backbone}} \text{语义表示}$
$[q_t, A_t^\tau] \xrightarrow{\text{Action Expert}} v_\theta(A_t^\tau, o_t) \xrightarrow{\text{积分}} A_t$

---

### 3.2 Flow Matching 动作生成

> **核心思想**: 用 Flow Matching 替代离散化，生成高精度连续动作分布

#### 关键公式解析

**公式 1: Flow Matching 训练损失**

$$
\mathcal{L}^\tau(\theta) = \mathbb{E}_{p(A_t|o_t), q(A_t^\tau|A_t)} \|v_\theta(A_t^\tau, o_t) - u(A_t^\tau|A_t)\|^2
$$

- **输入**:
  - $o_t$: 观察（图像 + 语言 + 状态）
  - $A_t$: ground truth 动作块，维度 $H \times d_a$
  - $\tau$: 流匹配时间步，$\tau \in [0, 1]$
- **计算过程**:
  1. 采样噪声 $\epsilon \sim \mathcal{N}(0, I)$
  2. 构造 noisy actions: $A_t^\tau = \tau A_t + (1-\tau)\epsilon$
  3. 计算目标向量场: $u(A_t^\tau|A_t) = \epsilon - A_t$
  4. 训练网络预测向量场: $v_\theta(A_t^\tau, o_t)$
- **输出**: MSE 损失，监督向量场预测
- **直觉理解**: 网络学习从任意噪声水平恢复到真实动作的"方向"

**公式 2: 推理时的积分过程**

$$
A_t^{\tau+\delta} = A_t^\tau + \delta \cdot v_\theta(A_t^\tau, o_t)
$$

- **输入**:
  - $A_t^0 \sim \mathcal{N}(0, I)$: 从标准高斯采样的初始噪声
  - $\delta = 0.1$: 积分步长
- **计算过程**:
  1. 从 $\tau = 0$ 开始，执行 10 步前向欧拉积分
  2. 每步用网络预测向量场 $v_\theta$，更新动作
- **输出**: $A_t^1 = A_t$，去噪后的动作块
- **直觉理解**: 从纯噪声"流动"到真实动作分布

**公式 3: 时间步采样分布（shifted beta）**

$$
p(\tau) = \text{Beta}\left(\frac{s-\tau}{s}; 1.5, 1\right), \quad s = 0.999
$$

- **输入**: 超参数 $s = 0.999$
- **计算过程**: 从变换后的 beta 分布采样 $\tau$
- **输出**: 偏向低时间步（高噪声）的 $\tau$ 值
- **直觉理解**: 与图像生成不同，机器人动作预测需要更多关注高噪声情况，因为 $\mathbb{E}[A_t|o_t]$ 本身就很难预测

---

### 3.3 Action Expert 设计

> **核心思想**: 用类似 MoE 的架构，让 VLM 和动作专家各司其职

#### 架构示意

**参考图**: Figure 3（中间部分）

```
┌─────────────────────────────────────────────────────┐
│            Mixture-of-Experts 式设计                 │
├─────────────────────────────────────────────────────┤
│                                                     │
│  Expert 1: VLM Backbone (Gemma 2.6B)               │
│  ├── 处理: 图像 tokens, 语言 tokens                 │
│  ├── 权重: 来自 PaliGemma 预训练                    │
│  └── 配置: width=2048, depth=18, heads=18          │
│                                                     │
│  Expert 2: Action Expert (300M)                    │
│  ├── 处理: 状态 q_t, 噪声动作 A_t^τ                 │
│  ├── 权重: 从头训练                                 │
│  └── 配置: width=1024, mlp_dim=4096                │
│                                                     │
│  交互方式: 仅通过 Self-Attention 层交互            │
│                                                     │
└─────────────────────────────────────────────────────┘
```

#### 关键组件

**组件 1: 动作输入投影**

- **作用**: 将噪声动作 + 时间步编码为 transformer embedding
- **实现**:
  ```
  embedding = W₃ · swish(W₂ · concat(W₁ · a_t^τ, φ(τ)))
  ```
  其中 $\phi(\tau)$ 是正弦位置编码

**组件 2: 注意力掩码设计**

| Block | 包含 tokens | 可以 attend 到 |
|-------|------------|---------------|
| Block 1 | $[I_t^1, ..., I_t^n, \ell_t]$ | 仅 Block 1 |
| Block 2 | $[q_t]$ | Block 1 + Block 2 |
| Block 3 | $[A_t^\tau]$ | 全部 |

- **设计理由**:
  - Block 1 保持 VLM 预训练分布，避免分布偏移
  - Block 2 缓存，因为状态在积分过程中不变
  - Block 3 需要全局信息来预测动作

**网络结构表**:

| 模块 | 参数量 | width | depth | heads | 功能 |
|-----|-------|-------|-------|-------|------|
| SigLIP | 400M | - | - | - | 图像编码 |
| Gemma (VLM) | 2.6B | 2048 | 18 | 18 | 语言理解 |
| Action Expert | 300M | 1024 | 18 | - | 动作生成 |
| **总计** | **3.3B** | - | - | - | - |

---

### 3.4 训练策略

#### 两阶段训练流程

| 阶段 | 数据 | 目标 | 训练步数 |
|-----|------|------|---------|
| **Pre-training** | OXE + 自有数据 (10,000+ hrs) | 广泛物理能力 | 700k steps |
| **Post-training** | 高质量任务特定数据 | 流畅执行特定任务 | 视任务而定 |

**预训练数据混合**:

| 数据源 | 规模 | 混合比例 | 说明 |
|-------|------|---------|------|
| 自有数据 | 903M timesteps | ~90.9% | 68 tasks, 7 robot configs |
| OXE Magic Soup | - | ~9.1% | 22 种机器人 |

**数据权重策略**:
- 每个 task-robot 组合的权重 = $n^{0.43}$，其中 $n$ 是样本数
- 过度采样的组合被降权，平衡数据分布

#### 跨具身处理

**动作/状态维度统一**:
- 最大维度: 18 (双臂 6×2 + 夹爪 2 + 移动底盘 2 + 升降 1)
- 低维机器人: 零填充
- 缺少相机: 掩码处理

**语言标签类型**:
- 任务名称: "fold the shirt"
- 片段标注: 约 2 秒长度的细粒度标签

---

### 3.5 推理流程

#### 推理时间分解

| 组件 | 时间 | 说明 |
|------|------|------|
| 图像编码器 | 14 ms | SigLIP 编码 3 张图像 |
| 观察前向传播 | 32 ms | VLM backbone |
| 10 步 Flow Matching | 27 ms | Action Expert ×10 |
| 网络延迟 (离板) | 13 ms | 可选 |
| **总计 (板载)** | **73 ms** | RTX 4090 |
| **总计 (离板)** | **86 ms** | 通过 WiFi |

**推理优化**:
- **KV 缓存**: 观察对应的 key/value 只计算一次，10 步积分复用
- **动作块执行**:
  - 20Hz 机器人: 每 0.8s 推理一次（执行 16 个动作）
  - 50Hz 机器人: 每 0.5s 推理一次（执行 25 个动作）

---

### 3.6 高层策略集成

> **核心思想**: 用高层 VLM 分解复杂任务为中间语言指令

```
High-Level Command: "bus the table"
         ↓
    [High-Level VLM]
         ↓
Intermediate: "pick up the napkin" → "throw it in the trash" → ...
         ↓
    [π₀ Low-Level Policy]
         ↓
    Continuous Actions
```

- **应用场景**: 桌面清理、杂货装袋等需要语义推理的任务
- **优势**: 利用 VLM 的语义理解能力进行任务规划

---

### 3.7 方法总结

#### 整体 Pipeline 流程表

| 阶段 | 模块 | 输入 | 输出 | 关键操作 |
|-----|------|------|------|---------|
| 1 | SigLIP | 多张 RGB 图像 | 图像 embeddings | 视觉编码 |
| 2 | Tokenizer | 语言指令 | 语言 tokens | 文本编码 |
| 3 | VLM Backbone | 图像+语言 tokens | 语义表示 | 多模态理解 |
| 4 | Action Expert | 状态+噪声动作 | 向量场 $v_\theta$ | Flow Matching |
| 5 | 积分器 | 10 步积分 | 动作块 $A_t$ | 欧拉积分 |

**数据流向总结**:

$$[I_t, \ell_t, q_t] \xrightarrow{\text{Encode}} [\text{embeddings}] \xrightarrow{\text{VLM+Expert}} v_\theta \xrightarrow{\text{Integrate}} A_t$$

#### 关键设计选择及理由

**设计 1: Flow Matching 替代离散化**
- **具体设计**: 用连续流匹配生成动作，而非离散 token
- **设计理由**: 离散化限制精度和控制频率
- **解决的问题**: 支持 50Hz 高频灵巧控制

**设计 2: Action Expert 分离**
- **具体设计**: 300M 独立参数处理动作，与 VLM 权重分离
- **设计理由**: VLM 权重适配视觉语言，动作需要独立学习
- **解决的问题**: 避免动作学习干扰 VLM 预训练表示

**设计 3: 偏向低时间步的采样**
- **具体设计**: 用 shifted beta 分布采样，强调高噪声
- **设计理由**: 动作预测比图像生成更难，需要更多高噪声训练
- **解决的问题**: 提升去噪质量

#### 与 baseline 的关键区别

- **vs OpenVLA**: 本文用 Flow Matching 替代离散化，支持 action chunking
- **vs Diffusion Policy**: 本文有 VLM 预训练，继承语义理解能力
- **vs Octo**: 本文规模更大 (3.3B vs 93M)，能力更强

---

## Section 4: Experiments

---

### 4.1 实验设置概述

#### 实验目的

验证以下核心问题：
1. π₀ 预训练后开箱即用 (zero-shot) 的能力如何？
2. π₀ 的语言指令跟随能力如何？
3. 微调到新任务时，与现有方法相比如何？
4. 在复杂多阶段任务上能达到什么水平？

#### 评估指标

- **任务进度分数 (Task Progress)**: 0-1 之间，1.0 表示完全成功
- 部分完成给予部分分数（如清理 7 个物品中的 5 个 = 5/7）

#### 机器人平台

| 平台 | 自由度 | 相机数 | 控制频率 | 说明 |
|------|--------|--------|----------|------|
| UR5e | 7 | 2 | 20Hz | 单臂 |
| Bimanual UR5e | 14 | 3 | 20Hz | 双臂 |
| Franka | 8 | 2 | 20Hz | 单臂 |
| Bimanual Trossen | 14 | 3 | 50Hz | 双臂 (ALOHA) |
| Bimanual ARX | 14 | 3 | 50Hz | 双臂 |
| Mobile Trossen | 16 | 3 | 50Hz | 移动双臂 |
| Mobile Fibocom | 17 | 3 | 50Hz | 全向移动双臂 |

#### 对比方法

| 方法 | 参数量 | 特点 |
|------|--------|------|
| **π₀** | 3.3B | VLM + Flow Matching |
| **π₀-small** | 470M | 无 VLM 预训练的消融版本 |
| **OpenVLA** | 7B | 开源 VLA，离散化动作 |
| **Octo** | 93M | 支持扩散的小型模型 |
| **ACT** | - | Action Chunking Transformer |
| **Diffusion Policy** | - | 扩散动作生成 |

---

### 4.2 开箱即用评估 (Out-of-Box)

#### 实验目的

> 评估 π₀ 预训练后，不经任何微调直接执行任务的能力

#### 实验设置

**任务列表** (Figure 6):

| 任务 | 机器人 | 描述 | 难点 |
|------|--------|------|------|
| Shirt Folding | Bi-ARX | 折叠 T 恤 | 柔性物体操作 |
| Bussing Easy | UR5e | 清理 7 个物品 | 物品分类 |
| Bussing Hard | UR5e | 清理 12 个物品 | 复杂遮挡配置 |
| Grocery Bagging | UR5e | 装袋 7 个杂货 | 物品多样性 |
| Toast | Bi-Trossen | 从烤面包机取出吐司 | 精细操作 |

**训练配置**:
- π₀: 700k steps 完整训练
- π₀ (parity): 160k steps，与 baseline 训练步数一致

#### Figure 7 分析

> **图示内容**: 5 个任务上不同方法的任务进度对比柱状图

#### 关键发现

| 对比项 | 数据 | 说明 |
|-------|------|------|
| π₀ vs OpenVLA (衬衫折叠) | 0.95 vs 0.10 (+850%) | 离散化 VLA 无法处理灵巧任务 |
| π₀ vs Octo (平均) | 0.80 vs 0.08 (+900%) | 规模和架构都重要 |
| π₀ vs π₀-small | 0.80 vs 0.47 (+70%) | VLM 预训练贡献显著 |
| π₀ vs π₀-parity | 0.80 vs 0.55 (+45%) | 更多训练步数有帮助 |
| OpenVLA (full) vs OpenVLA (UR5e only) | 任务依赖 | 跨具身训练对 OpenVLA 不利 |

**一句话总结**: π₀ 在所有开箱即用任务上大幅领先，证明了 Flow Matching + VLM 预训练的有效性。

---

### 4.3 语言指令跟随评估

#### 实验目的

> 评估 π₀ 理解和执行中间语言指令的能力

#### 实验设置

**任务** (Figure 8):

| 任务 | 物品数 | 指令数 | 描述 |
|------|--------|--------|------|
| Bussing | 12 | ~30 | 分类餐具和垃圾 |
| Table Setting | 7 | ~20 | 摆放餐具 |
| Grocery Bagging | 7 | ~14 | 按指令装袋 |

**评估条件**:
- **flat**: 仅给高层任务指令 ("bag the groceries")
- **human**: 人类提供中间指令 ("pick up the coffee")
- **HL**: 高层 VLM 自动提供中间指令

#### Figure 9 分析

> **图示内容**: π₀ 和 π₀-small 在不同指令条件下的性能对比

#### 关键发现

| 对比项 | 数据 | 说明 |
|-------|------|------|
| π₀-human vs π₀-flat | +30-40% | 中间指令显著提升性能 |
| π₀-HL vs π₀-flat | +15-25% | 自动高层策略有效 |
| π₀-human vs π₀-small-human | +25-35% | VLM 预训练提升语言理解 |
| π₀-small-HL vs π₀-small-flat | ~0% | 无 VLM 预训练则无法利用高层指令 |

**一句话总结**: VLM 预训练显著提升语言指令跟随能力，使得高层 VLM 策略可以有效辅助复杂任务。

---

### 4.4 微调评估 (Fine-tuning)

#### 实验目的

> 评估 π₀ 微调到新任务的能力，与现有灵巧操作方法对比

#### 实验设置

**任务难度分层**:

| 难度 | 任务 | 与预训练相似度 | 微调数据 |
|------|------|--------------|---------|
| Easy | Stack Bowls (UR5e) | 高 (类似 bussing) | 1-15 hrs |
| Easy | Towel Folding (Bi-ARX) | 高 (类似 shirt folding) | 1-15 hrs |
| Medium | Tupperware in Microwave | 中 (新物体) | 1-15 hrs |
| Hard | Paper Towel Replacement | 低 (全新动作) | 1-15 hrs |
| Hard | Franka Items in Drawer | 低 (新平台+新任务) | 1-15 hrs |

#### Figure 10 & 11 分析

> **图示内容**: 不同数据量下各方法的微调性能曲线

#### 关键发现

| 对比项 | 数据 | 说明 |
|-------|------|------|
| π₀ vs ACT (平均) | +20-40% | 预训练带来泛化优势 |
| π₀ vs Diffusion Policy | +15-30% | 结合 VLM 优于纯扩散 |
| π₀ vs OpenVLA/Octo | +50-80% | 基线在灵巧任务上表现差 |
| π₀ (pre-trained) vs π₀ (scratch) | +30-100% | 预训练对困难任务帮助更大 |
| 数据量 1hr vs 15hr | 任务依赖 | 简单任务 1hr 足够 |

**关键洞察**:
- 最强 baseline 往往是从头训练的方法 (ACT, DP)，说明现有预训练模型难以迁移到灵巧任务
- π₀ 的预训练在困难任务上优势更明显（有时提升 2x）
- 预训练模型在小数据量时优势更大

**一句话总结**: π₀ 在微调场景下持续优于所有方法，预训练对困难任务和小数据量场景尤其有效。

---

### 4.5 复杂多阶段任务

#### 实验目的

> 评估 π₀ 在最复杂、最长时间的任务上的表现

#### 实验设置

**任务列表** (Figure 12):

| 任务 | 时长 | 在预训练中 | 描述 | 评分满分 |
|------|------|-----------|------|---------|
| Laundry Folding | 5 min | ✓ | 从随机皱褶状态折叠多件衣物 | 4 |
| Mobile Laundry | 5 min | ✓ | 移动机器人折叠洗衣 | 4 |
| Dryer Unloading | 5 min | ✓ | 从烘干机卸载到篮子 | 5 |
| Table Bussing | 10 min | ✗ | 清理真实午餐桌 | 12 |
| Box Building | 5 min | ✗ | 组装纸板箱 | 5 |
| To-go Box | 5 min | ✗ | 将食物装入外卖盒 | 5 |
| Packing Eggs | 5 min | ✗ | 将鸡蛋装入蛋托并盖上 | 7 |

#### Figure 13 分析

> **图示内容**: 复杂任务上 π₀ 不同配置的性能对比柱状图

#### 关键发现

| 对比项 | 数据 | 说明 |
|-------|------|------|
| π₀ (fine-tuned) vs π₀ (scratch) | 洗衣折叠: 0.75 vs 0.35 (+114%) | 预训练对复杂任务至关重要 |
| π₀ (fine-tuned) vs π₀ (out-of-box) | 组装盒子: 0.55 vs N/A | 新任务需要微调 |
| π₀ 绝对性能 | 0.55-0.75 | 大部分复杂任务成功率 >50% |
| 预训练中任务 vs 新任务 | 预训练任务更高 | 符合预期 |

**任务难度分析**:
- **最难**: Box Building (0.55) - 需要复杂的双臂协调和物理推理
- **最简单**: Laundry Folding (0.75) - 预训练数据覆盖较好

**一句话总结**: π₀ 首次展示了端到端学习系统执行 5-20 分钟复杂灵巧任务的能力，代表了机器人学习的新里程碑。

---

### 4.6 消融实验

#### 实验目的

> 验证各设计选择的有效性

#### 核心消融发现

| 消融项 | 性能变化 | 结论 |
|-------|---------|------|
| 去掉 VLM 预训练 (π₀-small) | 从 0.80 降到 0.47 (-41%) | VLM 预训练至关重要 |
| 减少训练步数 (160k vs 700k) | 从 0.80 降到 0.55 (-31%) | 更多训练有帮助 |
| OpenVLA 跨具身训练 | 有时降低 | 离散化架构不适合跨具身 |

**π₀-small vs π₀ 差异**:
1. 无 VLM 预训练
2. 参数量更小 (470M vs 3.3B)
3. 使用 DiT 架构的 Action Expert
4. 语言用 DistilBERT 编码

**关键洞察**:
- **最重要的设计**: VLM 预训练，因为提供了语义理解和泛化能力
- **最 surprising 的发现**: OpenVLA 在跨具身训练后某些任务反而变差

---

## Section 5: Conclusion & Takeaways

---

### 5.1 核心贡献回顾

- **核心贡献**: 提出 π₀，首个结合 VLM 预训练和 Flow Matching 的机器人基础模型
- **主要优势**:
  1. 高频精确控制 (50Hz)
  2. 跨具身泛化 (7 种机器人)
  3. 复杂灵巧任务 (10-20 分钟长时程)
- **局限性**:
  1. 闭源：模型和数据都未公开
  2. 预训练数据组成的最优策略未充分研究
  3. 跨任务/机器人的正向迁移程度未完全理解

---

### 5.2 个人评价

#### 优点

1. **规模空前**: 10,000+ 小时机器人数据，目前最大规模的机器人学习实验
2. **任务复杂度高**: 首次展示端到端学习系统执行叠衣服、组装盒子等复杂任务
3. **架构设计合理**: Flow Matching 解决了离散化 VLA 的精度问题
4. **训练范式创新**: pre-training/post-training 分离借鉴 LLM 成功经验
5. **跨具身泛化**: 单一模型支持 7 种不同机器人配置

#### 不足

1. **闭源**: 模型和数据都未开放，无法复现
2. **硬件要求高**: 需要特定的机器人平台和大量算力
3. **泛化边界不清**: 不同任务/机器人之间的正向迁移程度未充分研究
4. **数据集细节缺失**: 68 个任务的具体定义和数据分布未详细说明

---

### 5.3 对后续研究的启发

- **VLM + Flow Matching** 是 VLA 的有效组合
- **大规模预训练** 对机器人学习同样有效
- **Pre-training/Post-training 范式** 可能是机器人基础模型的标准训练方式
- **Action Expert 的 MoE 设计** 思路值得借鉴

---

### 5.4 与后续工作的关系

| 版本 | 主要改进 | 关键创新 |
|------|---------|---------|
| **π₀** | 本文 | VLM + Flow Matching VLA |
| **π₀.₅** | 开放世界泛化 | 异构数据协同训练、分层推理 |
| **π₀.6** | 从经验中学习 | (待发布) |

---

### 5.5 关键数字速查表

| 指标 | 数值 | 说明 |
|------|------|------|
| 模型参数量 | 3.3B | VLM 3B + Action Expert 300M |
| 预训练数据 | 10,000+ 小时 | 903M timesteps 自有 + OXE |
| 机器人配置 | 7 种 | 单臂、双臂、移动 |
| 任务数量 | 68 | 预训练任务 |
| 控制频率 | 50 Hz | 灵巧操作 |
| Action Chunk | H=50 | 一次预测 50 步 |
| Flow Matching 步数 | 10 | 推理时积分步数 |
| 推理时间 | 73-86 ms | 板载/离板 |
| 衬衫折叠成功率 | ~95% | 开箱即用 |
| 洗衣折叠成功率 | ~75% | 微调后 |

---

*笔记创建时间: 2024-12*
*按 paper-reading skill 格式重做: 2025-12*
